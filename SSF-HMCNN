from __future__ import print_function, division, absolute_import
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.model_zoo as model_zoo
import os
import sys
import math
import pywt# 需要安装PyWavelets库
# IncLHQ; IncLHQnoBlkTwo; IncLHQ2BlkTwo; IncLHQ2BlkFour; IncLHQ3BlkFour; IncLHQ4BlkFour; IncLHQ5BlkFour;IncLHQ3x3;
# IncLHQcanshu; IncLHQ2Two4Four；IncLHQ1avg;IncLHQfca;IncLHQ2Two0Four
from Model_2D_Class.Inception.IncT1.IncLHQ import InceptionLHQ
# from Model_2D_Class.Inception.IncTWZ.IncLHQ4BlkFour import InceptionLHQ
# from Model_2D_Class.Inception.IncTWZrfb.resnet_mzy import resnet50不行

from Model_2D_Class.Inception.IncTWZ.ChannelSptailAttention import CBAM
from Model_2D_Class.Inception.IncTWZ.IncLHQ_4CBAM import InceptionLHQ_4CBAM

from Model_2D_Class.Inception.IncTWZ.selfAttention import SelfAttention

pretrained_settings = {
    'inceptionv4': {
        'imagenet': {
            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',
            'input_space': 'RGB',
            'input_size': [3, 299, 299],
            'input_range': [0, 1],
            'mean': [0.5, 0.5, 0.5],
            'std': [0.5, 0.5, 0.5],
            'num_classes': 1000
        },
        'imagenet+background': {
            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth',
            'input_space': 'RGB',
            'input_size': [3, 299, 299],
            'input_range': [0, 1],
            'mean': [0.5, 0.5, 0.5],
            'std': [0.5, 0.5, 0.5],
            'num_classes': 1001
        }
    }
}

class Mish(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x):
        x = x * (torch.tanh(F.softplus(x)))
        return x

def extract_wavelet_coefficients(images):
    wavelet = 'db4'  # 定义小波基函数
    batch_size, bands, height, width = images.shape  # 获取图像尺寸
    wavelet_coeffs = torch.zeros((batch_size, bands, height, width), dtype=torch.float32)  # 初始化小波系数张量

    # -----对每个图像执行二维离散小波变换-----
    for i in range(batch_size):
        for j in range(bands):
            image = images[i, j]  # 获取图像
            image_numpy = image.detach().cpu().numpy()
             # 将输入张量从CUDA设备移动到主机内存才能运算
            coeffs = pywt.dwt2(image_numpy, wavelet, mode='zero', )  # 执行二维离散小波变换，使用对称填充模式mode='symmetric'，常数填充
            # print(coeffs[0].shape)  # 打印小波变换结果的形状
            coeffs_shape = coeffs[0].shape  # 获取变换结果的尺寸
            wavelet_coeffs[i, j, :coeffs_shape[0], :coeffs_shape[1]] = torch.from_numpy(coeffs[0])  # 将小波系数存储到结果张量中
    return wavelet_coeffs

class BasicConv2d(nn.Module):

    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_planes, out_planes,
                              kernel_size=kernel_size, stride=stride,
                              padding=padding, bias=False) # verify bias false
        self.bn = nn.BatchNorm2d(out_planes,
                                 eps=0.001, # value found in tensorflow
                                 momentum=0.1, # default pytorch value
                                 affine=True)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x
class BasicConv(nn.Module):

    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):
        super(BasicConv, self).__init__()
        self.out_channels = out_planes
        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)
        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None
        self.relu = nn.ReLU(inplace=True) if relu else None

    def forward(self, x):
        x = self.conv(x)
        if self.bn is not None:
            x = self.bn(x)
        if self.relu is not None:
            x = self.relu(x)
        return x

class BasicRFB(nn.Module):

    def __init__(self, in_planes = 40, out_planes = 40, stride=1, scale = 0.1, visual = 1):
        super(BasicRFB, self).__init__()
        self.scale = scale
        self.out_channels = out_planes
        inter_planes = in_planes // 4
        self.branch0 = nn.Sequential(
                BasicConv(in_planes, 2*inter_planes, kernel_size=1, stride=stride),
                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=visual, dilation=visual, relu=False)
                )
        self.branch1 = nn.Sequential(
                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),
                BasicConv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),
                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=visual+1, dilation=visual+1, relu=False)
                )
        self.branch2 = nn.Sequential(
                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),
                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),
                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=3, stride=stride, padding=1),
                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=2*visual+1, dilation=2*visual+1, relu=False)
                )

        self.ConvLinear = BasicConv(6*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)
        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)
        self.relu = nn.ReLU(inplace=False)

    def forward(self,x):
        x0 = self.branch0(x)
        x1 = self.branch1(x)
        x2 = self.branch2(x)

        out = torch.cat((x0,x1,x2),1)
        out = self.ConvLinear(out)
        short = self.shortcut(x)
        out = out*self.scale + short
        out = self.relu(out)

        return out

# ------------FCAnet频域注意力----------------
class ConvBNRelu(nn.Module):
    def __init__(self, in_planes, out_planes=16, kernelsz=3, strides=1, padding=1):
        super(ConvBNRelu, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(in_channels=in_planes, out_channels=out_planes, kernel_size=kernelsz, stride=strides,
                      padding=padding),
            nn.BatchNorm2d(out_planes),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.model(
            x)  # 在training=False时，BN通过整个训练集计算均值、方差去做批归一化，training=True时，通过当前batch的均值、方差去做批归一化。推理时 training=False效果好
        return x
def get_freq_indices(method):
    assert method in ['top1', 'top2', 'top4', 'top8', 'top16', 'top32',
                      'bot1', 'bot2', 'bot4', 'bot8', 'bot16', 'bot32',
                      'low1', 'low2', 'low4', 'low8', 'low16', 'low32']
    num_freq = int(method[3:])
    if 'top' in method:
        all_top_indices_x = [0, 0, 6, 0, 0, 1, 1, 4, 5, 1, 3, 0, 0, 0, 3, 2, 4, 6, 3, 5, 5, 2, 6, 5, 5, 3, 3, 4, 2, 2,
                             6, 1]
        all_top_indices_y = [0, 1, 0, 5, 2, 0, 2, 0, 0, 6, 0, 4, 6, 3, 5, 2, 6, 3, 3, 3, 5, 1, 1, 2, 4, 2, 1, 1, 3, 0,
                             5, 3]
        mapper_x = all_top_indices_x[:num_freq]
        mapper_y = all_top_indices_y[:num_freq]
    elif 'low' in method:
        all_low_indices_x = [0, 0, 1, 1, 0, 2, 2, 1, 2, 0, 3, 4, 0, 1, 3, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 1, 2,
                             3, 4]
        all_low_indices_y = [0, 1, 0, 1, 2, 0, 1, 2, 2, 3, 0, 0, 4, 3, 1, 5, 4, 3, 2, 1, 0, 6, 5, 4, 3, 2, 1, 0, 6, 5,
                             4, 3]
        mapper_x = all_low_indices_x[:num_freq]
        mapper_y = all_low_indices_y[:num_freq]
    elif 'bot' in method:
        all_bot_indices_x = [6, 1, 3, 3, 2, 4, 1, 2, 4, 4, 5, 1, 4, 6, 2, 5, 6, 1, 6, 2, 2, 4, 3, 3, 5, 5, 6, 2, 5, 5,
                             3, 6]
        all_bot_indices_y = [6, 4, 4, 6, 6, 3, 1, 4, 4, 5, 6, 5, 2, 2, 5, 1, 4, 3, 5, 0, 3, 1, 1, 2, 4, 2, 1, 1, 5, 3,
                             3, 3]
        mapper_x = all_bot_indices_x[:num_freq]
        mapper_y = all_bot_indices_y[:num_freq]
    else:
        raise NotImplementedError
    return mapper_x, mapper_y
class MultiSpectralAttentionLayer(torch.nn.Module):
    def __init__(self, channel, dct_h, dct_w, reduction=16, freq_sel_method='top16'):
        super(MultiSpectralAttentionLayer, self).__init__()
        self.reduction = reduction
        self.dct_h = dct_h
        self.dct_w = dct_w

        mapper_x, mapper_y = get_freq_indices(freq_sel_method)
        self.num_split = len(mapper_x)
        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x]
        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]
        # make the frequencies in different sizes are identical to a 7x7 frequency space
        # eg, (2,2) in 14x14 is identical to (1,1) in 7x7

        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, channel)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        n, c, h, w = x.shape
        x_pooled = x
        if h != self.dct_h or w != self.dct_w:
            x_pooled = torch.nn.functional.adaptive_avg_pool2d(x, (self.dct_h, self.dct_w))
            # If you have concerns about one-line-change, don't worry.   :)
            # In the ImageNet models, this line will never be triggered.
            # This is for compatibility in instance segmentation and object detection.
        y = self.dct_layer(x_pooled)

        y = self.fc(y).view(n, c, 1, 1)
        return x * y.expand_as(x)
class MultiSpectralDCTLayer(nn.Module):
    """
    Generate dct filters
    """

    def __init__(self, height, width, mapper_x, mapper_y, channel):
        super(MultiSpectralDCTLayer, self).__init__()

        assert len(mapper_x) == len(mapper_y)
        assert channel % len(mapper_x) == 0

        self.num_freq = len(mapper_x)

        # fixed DCT init
        self.register_buffer('weight', self.get_dct_filter(height, width, mapper_x, mapper_y, channel))

        # fixed random init
        # self.register_buffer('weight', torch.rand(channel, height, width))

        # learnable DCT init
        # self.register_parameter('weight', self.get_dct_filter(height, width, mapper_x, mapper_y, channel))

        # learnable random init
        # self.register_parameter('weight', torch.rand(channel, height, width))

        # num_freq, h, w

    def forward(self, x):
        assert len(x.shape) == 4, 'x must been 4 dimensions, but got ' + str(len(x.shape))
        # n, c, h, w = x.shape

        x = x * self.weight

        result = torch.sum(x, dim=[2, 3])
        return result

    def build_filter(self, pos, freq, POS):
        result = math.cos(math.pi * freq * (pos + 0.5) / POS) / math.sqrt(POS)
        if freq == 0:
            return result
        else:
            return result * math.sqrt(2)

    def get_dct_filter(self, tile_size_x, tile_size_y, mapper_x, mapper_y, channel):
        dct_filter = torch.zeros(channel, tile_size_x, tile_size_y)

        c_part = channel // len(mapper_x)

        for i, (u_x, v_y) in enumerate(zip(mapper_x, mapper_y)):
            for t_x in range(tile_size_x):
                for t_y in range(tile_size_y):
                    dct_filter[i * c_part: (i + 1) * c_part, t_x, t_y] = self.build_filter(t_x, u_x,
                        tile_size_x) * self.build_filter(t_y, v_y, tile_size_y)

        return dct_filter
# -------------------------------------------


class F3D2D_MCNet(nn.Module):
    def __init__(self, in_channels):
        super(F3D2D_MCNet, self).__init__()

        # self.BAM = ChannelAttention(in_channels) #确定是40个
        # self.SAM = SpatialAttention(in_channels,in_channels)

        self.convReduction1 = nn.Conv3d(in_channels=1, out_channels=1, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=0)
        self.xBasicRFB = BasicRFB()

        # -----波段特征残差块-----
        self.bam1 = nn.Sequential(
            nn.Conv3d(in_channels=1, out_channels=12, kernel_size=(3, 1, 1), stride=(3, 1, 1)),
            nn.BatchNorm3d(12),
            nn.ReLU(),
        )

        self.bam2 = nn.Sequential(
            nn.Conv3d(in_channels=12, out_channels=12, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0)),
            nn.BatchNorm3d(12),
            nn.ReLU(),

            nn.Conv3d(in_channels=12, out_channels=12, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0)),
            nn.BatchNorm3d(12),
            nn.ReLU(),
        )

        self.bam3 = nn.Sequential(
            nn.Conv3d(in_channels=12, out_channels=12,
                kernel_size=(13, 1, 1), #原先是1,1。65像素的输出时为：2、2
                #stride=(1, 2, 2), # 原先没有，65的输出时stride=(1, 2, 2)
            ),
            nn.BatchNorm3d(12),
            nn.ReLU(),
        )

        #------------------------------------

        self.convReduction2 = nn.Sequential(
            nn.Conv2d(in_channels=40, out_channels=32, kernel_size=(1, 1), stride=(1, 1), padding=0),
            nn.BatchNorm2d(32),
            nn.ReLU(),
        ) # 输出127x127x16


        #----------------------------------
        self.c5_1 = ConvBNRelu(in_planes=40, out_planes=12, kernelsz=1, strides=1, padding=0)
        self.FcaNet = MultiSpectralAttentionLayer(40, dct_h=127, dct_w=127, reduction=2,
                                              freq_sel_method="low2")  # h=64,w=64
        #----------------------------------

        # -----空间特征残差块-----
        # self.sam1 = nn.Sequential(
        #     nn.Conv3d(in_channels=12, out_channels=12, kernel_size=(in_channels, 1, 1), stride=(1, 1, 1)),
        #     nn.BatchNorm3d(12),
        #     Mish(),
        # )

        self.sam2 = nn.Sequential(
            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, padding=1),
            nn.BatchNorm2d(12),
            nn.ReLU(),

            nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, padding=1),
            nn.BatchNorm2d(12),
            nn.ReLU(),
        )


    def forward(self, x):
        ## 波段注意力和空间注意力结合方式
        # out = self.BAM(x)
        # out = self.SAM(out)
        #-----当不用注意力机制时，直接加此代码，其他关于注意力机制的注释掉即可-----
        b, c, h, w = x.shape
        x = x.reshape(b, 1, c, h, w)
        #--------------------------------------------------------------
        x = self.convReduction1(x)

        # ----光谱特征提取模块运算-----
        out1 = self.bam1(x)
        out1 = out1 + self.bam2(out1)

        out1 = self.bam3(out1)
        b, d, c, h, w = out1.shape
        out1 = out1.reshape(b, d, h, w)

        b, d, c, h, w = x.shape
        x = x.reshape(b, c, h, w)

        # ------空间残差----------------------
        x5 = self.c5_1(x)
        # out2 = self.sam1(x5)
        # b, d, c, h, w = out2.shape
        # out2 = out2.reshape(b, d, h, w)
        out2 = x5 - self.sam2(x5)
        # ------空间残差----------------------

        # x5_1 = self.FcaNet(x)
        # x5 = self.c5_1(x5_1)
        # wavelet_coeffs = extract_wavelet_coefficients(x).cuda()
        # x = self.xBasicRFB(x)
        Xcon2 = self.convReduction2(x)

        # out3 = out1 + out2
        out = torch.cat((Xcon2, out1, out2), dim=1) #out1,  out2,x5
        b, c, h, w = out.shape
        out = out.reshape(b, c, h, w)
        return out
# 输出127x127x48

class IncTWZ_CBAM(nn.Module):

    def __init__(self, num_classes=1001):
        super(IncTWZ_CBAM, self).__init__()
        # Special attributs
        self.input_space = None
        self.input_size = (127, 127, 32)
        self.mean = None
        self.std = None
        # Modules
        self.f3d2d_mcnet = F3D2D_MCNet(in_channels=40)

        self.features = InceptionLHQ()  # 刘汉青部分

        self.CBAM = CBAM(in_channels=256)  # CBAM

        #self.selfAttention = SelfAttention(in_channels=256)

        self.last_linear = nn.Linear(256, num_classes)

    def logits(self, features):
        #Allows image of any size to be processed
        adaptiveAvgPoolWidth = features.shape[2]
        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)
        x = x.view(x.size(0), -1)
        x = self.last_linear(x)
        return x

    def forward(self, input):
        x = self.f3d2d_mcnet(input)
        x = self.features(x)
        x = self.CBAM(x)
        #x = self.selfAttention(x)
        x = self.logits(x)
        return x

class IncTWZ_SelfAttention(nn.Module):

    def __init__(self, num_classes=1001):
        super(IncTWZ_SelfAttention, self).__init__()
        # Special attributs
        self.input_space = None
        self.input_size = (127, 127, 64)
        self.mean = None
        self.std = None
        # Modules
        self.f3d2d_mcnet = F3D2D_MCNet(in_channels=40)

        self.features = InceptionLHQ()  # 刘汉青部分

        self.selfAttention = SelfAttention(in_channels=256)

        self.last_linear = nn.Linear(256, num_classes)

    def logits(self, features):
        #Allows image of any size to be processed
        adaptiveAvgPoolWidth = features.shape[2]
        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)
        x = x.view(x.size(0), -1)
        x = self.last_linear(x)
        return x

    def forward(self, input):
        x = self.f3d2d_mcnet(input)
        x = self.features(x)
        x = self.selfAttention(x)
        x = self.logits(x)
        return x

class IncTWZ_4CBAM(nn.Module):

    def __init__(self, num_classes=1001):
        super(IncTWZ_4CBAM, self).__init__()
        # Special attributs
        self.input_space = None
        self.input_size = (127, 127, 64)
        self.mean = None
        self.std = None
        # Modules
        self.f3d2d_mcnet = F3D2D_MCNet(in_channels=40)

        self.features = InceptionLHQ_4CBAM()  # 刘汉青部分

        #self.selfAttention = SelfAttention(in_channels=256)

        self.last_linear = nn.Linear(256, num_classes)

    def logits(self, features):
        #Allows image of any size to be processed
        adaptiveAvgPoolWidth = features.shape[2]
        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)
        x = x.view(x.size(0), -1)
        x = self.last_linear(x)
        return x

    def forward(self, input):
        x = self.f3d2d_mcnet(input)
        x = self.features(x)
        x = self.logits(x)
        return x

class IncTWZ(nn.Module):

    def __init__(self, num_classes=1001):
        super(IncTWZ, self).__init__()
        # Special attributs
        self.input_space = None
        self.input_size = (127, 127, 64)
        self.mean = None
        self.std = None
        # Modules
        self.f3d2d_mcnet = F3D2D_MCNet(in_channels=40)

        self.features = InceptionLHQ()  # 刘汉青部分

        # self.CBAM = CBAM(in_channels=256)  # CBAM

        #self.selfAttention = SelfAttention(in_channels=256)

        self.last_linear = nn.Linear(256, num_classes)

    def logits(self, features):
        #Allows image of any size to be processed
        adaptiveAvgPoolWidth = features.shape[2]
        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)
        x = x.view(x.size(0), -1)
        x = self.last_linear(x)
        return x

    def forward(self, input):
        x = self.f3d2d_mcnet(input)
        x = self.features(x)
        # x = self.CBAM(x)
        #x = self.selfAttention(x)
        x = self.logits(x)
        return x



 # 以下是测试代码
# def Inc_TWZ(num_classes=5, pretrained='imagenet'):
#     if pretrained:
#         settings = pretrained_settings['inceptionv4'][pretrained]
#         assert num_classes == settings['num_classes'], \
#             "num_classes should be {}, but is {}".format(settings['num_classes'], num_classes)
#
#         # both 'imagenet'&'imagenet+background' are loaded from same parameters
#         model = IncTWZ_CBAM(num_classes=5)
#         model.load_state_dict(model_zoo.load_url(settings['url']))
#
#         if pretrained == 'imagenet':
#             new_last_linear = nn.Linear(1536, 1000)
#             new_last_linear.weight.data = model.last_linear.weight.data[1:]
#             new_last_linear.bias.data = model.last_linear.bias.data[1:]
#             model.last_linear = new_last_linear
#
#         model.input_space = settings['input_space']
#         model.input_size = settings['input_size']
#         model.input_range = settings['input_range']
#         model.mean = settings['mean']
#         model.std = settings['std']
#     else:
#         model = IncTWZ(num_classes=num_classes)
#     return model


if __name__ == '__main__':
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    tensor = torch.randn(8, 40, 256, 256).to(device)
    twz = IncTWZ_CBAM(num_classes=5).to(device)
    twz(tensor)
    print('success')
    # assert inceptionv4(num_classes=1000, pretrained='imagenet')
    # print('success')
    # assert inceptionv4(num_classes=1001, pretrained='imagenet+background')
    # print('success')
    #
    # # fail
    # assert inceptionv4(num_classes=1001, pretrained='imagenet')

# model = inceptionv4(num_classes=5, pretrained=None)
# # 打印模型的网络结构
# print(model)
